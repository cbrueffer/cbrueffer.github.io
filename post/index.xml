<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Christian Brueffer</title><link>https://www.brueffer.io/post/</link><atom:link href="https://www.brueffer.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 19 Jul 2022 00:00:00 +0000</lastBuildDate><image><url>https://www.brueffer.io/media/icon_hu7422bdd1ad1d5b8840d6088d6ba7ed4c_15338_512x512_fill_lanczos_center_3.png</url><title>Posts</title><link>https://www.brueffer.io/post/</link></image><item><title>Snakemake on AWS ParallelCluster using Slurm</title><link>https://www.brueffer.io/post/snakemake-aws-parallelcluster/</link><pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/post/snakemake-aws-parallelcluster/</guid><description>
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#tldr">tl;dr&lt;/a>&lt;/li>
&lt;li>&lt;a href="#running-snakemake-on-aws-parallelcluster">Running Snakemake on AWS ParallelCluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#aws-parallelcluster">AWS ParallelCluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#pros-and-cons">Pros and Cons&lt;/a>&lt;/li>
&lt;li>&lt;a href="#setup">Setup&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#snakemake-on-parallelcluster-with-slurm">Snakemake on ParallelCluster with Slurm&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#peculiarities-of-parallelcluster-slurm-instances">Peculiarities of ParallelCluster Slurm Instances&lt;/a>&lt;/li>
&lt;li>&lt;a href="#snakemake-slurm-execution-profiles">Snakemake Slurm Execution Profiles&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#other-tidbits-and-lessons-learned">Other Tidbits and Lessons Learned&lt;/a>&lt;/li>
&lt;li>&lt;a href="#resources">Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="#edit-history">Edit History&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="tldr">tl;dr&lt;/h2>
&lt;ul>
&lt;li>AWS ParallelCluster is a great way to run traditional HPC workflows in the cloud.&lt;/li>
&lt;li>AWS ParallelCluster Slurm has a few peculiarities in the default configuration.&lt;/li>
&lt;li>For running Snakemake an adapted profile is available at &lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm&lt;/a> to make things work smoothly.&lt;/li>
&lt;/ul>
&lt;h2 id="running-snakemake-on-aws-parallelcluster">Running Snakemake on AWS ParallelCluster&lt;/h2>
&lt;p>&lt;a href="https://snakemake.github.io/" target="_blank" rel="noopener">Snakemake&lt;/a> is a fantastic tool for orchestrating bioinformatics workflows and comes with &lt;a href="https://snakemake.readthedocs.io/en/stable/executing/cloud.html" target="_blank" rel="noopener">built-in support&lt;/a> for a variety of public cloud providers, such as Amazon AWS.
However, running a workflow this way can be a challenge, not least since hands-on tutorials or more practical information are rare.
An alternative that many bioinformaticians will be familiar with are high-performance computing (HPC) clusters using a job scheduler such as Slurm, since many universities and research facilities operate such systems on their own premises.&lt;/p>
&lt;h2 id="aws-parallelcluster">AWS ParallelCluster&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/parallelcluster" target="_blank" rel="noopener">AWS ParallelCluster&lt;/a> makes it possible and relatively easy to replicate such a setting in the cloud.
ParallelCluster is essentially a wrapper around more basic AWS services such as EC2 (running virtual machines), EFS and EBS (storage), and others.&lt;/p>
&lt;h3 id="pros-and-cons">Pros and Cons&lt;/h3>
&lt;p>Pros&lt;/p>
&lt;ul>
&lt;li>Feasible to set up for a single user or small company / research group without huge resources.&lt;/li>
&lt;li>It scales dynamically: nodes are allocated when needed and decommissioned when unused.&lt;/li>
&lt;li>Practically infinite resources: you can allocate as many nodes and use as much storage as you want.&lt;/li>
&lt;li>It makes it easy to transfer a workflow from a traditional on-premise HPC setting into the cloud.&lt;/li>
&lt;/ul>
&lt;p>Cons&lt;/p>
&lt;ul>
&lt;li>In a university setting you can often get time on HPC systems for free, only requiring a short project description.
Cloud-based HPC will cost you actual money.&lt;/li>
&lt;li>Traditional HPC clusters have qualified systems and network administrators that keep things running smoothly.
With ParallelCluster &lt;em>you&lt;/em> are the systems and network administrator, for better or worse.
While many things are automated, Amazon AWS is nothing if not complex and it still requires knowledge to make things work.&lt;/li>
&lt;/ul>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;p>The first steps of using ParallelCluster are &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html" target="_blank" rel="noopener">pretty well documented&lt;/a>.
All tasks are performed with the &lt;code>aws-parallelcluster&lt;/code> Python package.
Note that there are currently two versions of the ParallelCluster software, v2 and v3.
v3 is newer and has more features, but many guides and help you currently find online are based on v2.
The most visual difference is the configuration format: v3 uses &lt;a href="https://en.wikipedia.org/wiki/YAML" target="_blank" rel="noopener">YAML&lt;/a> while v2 uses &lt;a href="https://en.wikipedia.org/wiki/INI_file" target="_blank" rel="noopener">INI&lt;/a> files.&lt;/p>
&lt;ul>
&lt;li>Configure an AWS user with the correct permissions.
A basic policy is &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-example-user-policies" target="_blank" rel="noopener">provided in the AWS documentation&lt;/a>, but
if you need access to services such as EFS more permissions will be needed.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html" target="_blank" rel="noopener">Set up the AWS user&amp;rsquo;s credentials&lt;/a> using &lt;code>aws configure&lt;/code>.&lt;/li>
&lt;li>Either import an SSH public key in the EC2 settings, or generate a new key pair.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-parallelcluster.html" target="_blank" rel="noopener">Install the &lt;code>aws-parallelcluster&lt;/code>&lt;/a> Python package using &lt;code>pip&lt;/code> as documented.
The package is also available through &lt;code>conda&lt;/code> via the conda-forge channel, but as of this writing it is v2 only.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html" target="_blank" rel="noopener">Set up a cluster configuration&lt;/a> using &lt;code>pcluster configure --config my-cluster-config.yaml&lt;/code>.
This starts an interactive process that various settings of the cluster, i.e. which instance type to use for the head node, which scheduler to use (Slurm or AWS Batch), how many queues, number of nodes per queue, etc.
In particular sets up the necessary virtual private cloud (VPC), network infrastructure, and security groups on AWS automatically, if you tell it to do so.&lt;/li>
&lt;li>Start the cluster using &lt;code>pcluster create-cluster --cluster-name test-cluster --cluster-configuration my-cluster-config.yaml&lt;/code>.
This will take a few minutes; you can check progress using &lt;code>pcluster describe-cluster --cluster-name test-cluster&lt;/code>.&lt;/li>
&lt;li>Log in to the head node using &lt;code>pcluster ssh --cluster-name test-cluster&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="snakemake-on-parallelcluster-with-slurm">Snakemake on ParallelCluster with Slurm&lt;/h2>
&lt;h3 id="peculiarities-of-parallelcluster-slurm-instances">Peculiarities of ParallelCluster Slurm Instances&lt;/h3>
&lt;p>Generally using Snakemake on a ParallelCluster Slurm cluster works the same as with any other Slurm system.
That said, there are some differences and drawbacks, and they are not well documented and as such, are learned through the usual cycle of trial, error, and searching the web.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;code>sbatch&lt;/code> &lt;code>--mem&lt;/code> argument for specifying memory requirements for a job is not supported, and if used will send nodes into the &lt;code>DRAINED&lt;/code> state.
The company Ronin has a good &lt;a href="https://blog.ronin.cloud/slurm-parallelcluster-troubleshooting/" target="_blank" rel="noopener">troubleshooting page&lt;/a> on this topic.
If you are the sole user of your cluster the easiest workaround is probably to scale jobs using the number of CPUs (&lt;code>--cpus-per-task&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>By default, a ParallelCluster comes without working job accounting, which means &lt;code>sacct&lt;/code> does not work.
This is particularly noteworthy since many Snakemake Slurm profiles use &lt;code>sacct&lt;/code> for job monitoring.
A guide for setting up Slurm accounting on ParallelCluster is &lt;a href="https://aws.amazon.com/blogs/compute/enabling-job-accounting-for-hpc-with-aws-parallelcluster-and-amazon-rds/" target="_blank" rel="noopener">available from Amazon&lt;/a>.
If you are going to use ParallelCluster regularly it probably makes sense to set it up, since the underlying database will persist across cluster instances.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="snakemake-slurm-execution-profiles">Snakemake Slurm Execution Profiles&lt;/h3>
&lt;p>The profile I use is freely available on Github at &lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm&lt;/a> and accomodates the ParallelCluster peculiarities.
It is heavily based on &lt;a href="https://github.com/jdblischak/smk-simple-slurm" target="_blank" rel="noopener">John Blischak&amp;rsquo;s smk-simple-slurm repository&lt;/a> and carries the same license.&lt;/p>
&lt;p>Other profiles, such as the &lt;a href="https://github.com/Snakemake-Profiles/slurm" target="_blank" rel="noopener">standard Snakemake Slurm profile&lt;/a> do not currently work in this setting out of the box.&lt;/p>
&lt;h2 id="other-tidbits-and-lessons-learned">Other Tidbits and Lessons Learned&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>There is a &lt;a href="https://github.com/aws-samples/pcluster-manager" target="_blank" rel="noopener">graphical frontend to ParallelCluster&lt;/a>!
It is relatively new and not particularly well advertised in the documentation, but it looks useful and can help avoid potential configuration issues mentioned in this section.
It also simplifies setting up Slurm accounting.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Shared storage&lt;/p>
&lt;ul>
&lt;li>&lt;code>/home&lt;/code> is shared between the head node and the compute nodes.&lt;/li>
&lt;li>&lt;code>/scratch&lt;/code> is the local storage of each node.&lt;/li>
&lt;li>Other shared storage has to be explicity added to the configuration in ParallelCluster v3 (as opposed to v2 where &lt;code>/shared&lt;/code> is a shared EBS volume).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Head node instance type&lt;/p>
&lt;p>Give some thought to which instance type to select for the head node to best balance cost and performance.
Since it is the one cluster node that will run for as long as the cluster is up, the instance type should not be too expensive.
On the other hand it needs to be performant enough for Snakemake, job scheduling and NFS server duties, so do not select an instance type that is too underpowered.
For my applications &lt;code>t3.xlarge&lt;/code> was a good compromise.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check whether the instance types you select exist in your configured AWS region.
Not all instance types are available in all regions, and &lt;code>pcluster&lt;/code> will not warn you about this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check your limits!&lt;/p>
&lt;p>AWS services that ParallelCluster uses may have limits in place, including limits on concurrent EC2 instances and virtual CPUs.
Again, &lt;code>pcluster&lt;/code> will not warn you about this.
In practice this means you can configure and start a cluster that could scale up to hundreds of compute nodes, but when you start a job only a few
nodes spin up while the rest of your jobs are stuck waiting for resources and you are left wondering why.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If things do not work, check the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting-v3.html" target="_blank" rel="noopener">troubleshooting documentation&lt;/a> and the logs.&lt;/p>
&lt;p>On the head node:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">/var/log/cfn-init.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/chef-client.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/slurm_resume.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/slurm_suspend.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/clustermgtd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/slurmctld.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>On compute nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">/var/log/cloud-init-output.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/computemgtd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/slurmd.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Ephemeral storage &lt;strong>is ephemeral&lt;/strong>!&lt;/p>
&lt;p>All storage that is created during cluster creation will also be destroyed when the cluster is deleted.
This includes additional EFS and EBS storage defined in the cluster config, unless you explicitly create them yourself first and then include them e.g. via their &lt;code>FileSystemId&lt;/code> for EFS file systems!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multi-queue clusters&lt;/p>
&lt;p>The &lt;code>pcluster config&lt;/code> wizard enables creating ParallelCluster Slurm clusters with multiple queues.
Keep it mind that it is only possible to specify one instance type per queue.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">Snakemake profile for AWS ParallelCluster with Slurm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster" target="_blank" rel="noopener">ParallelCluster documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aws-parallelcluster.readthedocs.io" target="_blank" rel="noopener">pcluster package documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws-samples/pcluster-manager" target="_blank" rel="noopener">ParallelCluster graphical frontend&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hpc.news/techshorts" target="_blank" rel="noopener">Amazon HPC Tech Shorts Youtube channel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=PChP3FQWeJQ" target="_blank" rel="noopener">Amazon HPC Tech Shorts on pcluster manager GUI&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=a-99esKLcls" target="_blank" rel="noopener">Amazon HPC Tech Shorts on pcluster v3&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="edit-history">Edit History&lt;/h2>
&lt;ul>
&lt;li>2022-07-19 Initial version&lt;/li>
&lt;/ul></description></item></channel></rss>