<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open Source Software | Christian Brueffer</title><link>https://www.brueffer.io/tag/open-source-software/</link><atom:link href="https://www.brueffer.io/tag/open-source-software/index.xml" rel="self" type="application/rss+xml"/><description>Open Source Software</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 19 Jul 2022 00:00:00 +0000</lastBuildDate><image><url>https://www.brueffer.io/media/icon_hu7422bdd1ad1d5b8840d6088d6ba7ed4c_15338_512x512_fill_lanczos_center_3.png</url><title>Open Source Software</title><link>https://www.brueffer.io/tag/open-source-software/</link></image><item><title>Snakemake on AWS ParallelCluster using Slurm</title><link>https://www.brueffer.io/post/snakemake-aws-parallelcluster/</link><pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/post/snakemake-aws-parallelcluster/</guid><description>
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#tldr">tl;dr&lt;/a>&lt;/li>
&lt;li>&lt;a href="#running-snakemake-on-aws-parallelcluster">Running Snakemake on AWS ParallelCluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#aws-parallelcluster">AWS ParallelCluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#pros-and-cons">Pros and Cons&lt;/a>&lt;/li>
&lt;li>&lt;a href="#setup">Setup&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#snakemake-on-parallelcluster-with-slurm">Snakemake on ParallelCluster with Slurm&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#peculiarities-of-parallelcluster-slurm-instances">Peculiarities of ParallelCluster Slurm Instances&lt;/a>&lt;/li>
&lt;li>&lt;a href="#snakemake-slurm-execution-profiles">Snakemake Slurm Execution Profiles&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#other-tidbits-and-lessons-learned">Other Tidbits and Lessons Learned&lt;/a>&lt;/li>
&lt;li>&lt;a href="#resources">Resources&lt;/a>&lt;/li>
&lt;li>&lt;a href="#edit-history">Edit History&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="tldr">tl;dr&lt;/h2>
&lt;ul>
&lt;li>AWS ParallelCluster is a great way to run traditional HPC workflows in the cloud.&lt;/li>
&lt;li>AWS ParallelCluster Slurm has a few peculiarities in the default configuration.&lt;/li>
&lt;li>For running Snakemake an adapted profile is available at &lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm&lt;/a> to make things work smoothly.&lt;/li>
&lt;/ul>
&lt;h2 id="running-snakemake-on-aws-parallelcluster">Running Snakemake on AWS ParallelCluster&lt;/h2>
&lt;p>&lt;a href="https://snakemake.github.io/" target="_blank" rel="noopener">Snakemake&lt;/a> is a fantastic tool for orchestrating bioinformatics workflows and comes with &lt;a href="https://snakemake.readthedocs.io/en/stable/executing/cloud.html" target="_blank" rel="noopener">built-in support&lt;/a> for a variety of public cloud providers, such as Amazon AWS.
However, running a workflow this way can be a challenge, not least since hands-on tutorials or more practical information are rare.
An alternative that many bioinformaticians will be familiar with are high-performance computing (HPC) clusters using a job scheduler such as Slurm, since many universities and research facilities operate such systems on their own premises.&lt;/p>
&lt;h2 id="aws-parallelcluster">AWS ParallelCluster&lt;/h2>
&lt;p>&lt;a href="https://docs.aws.amazon.com/parallelcluster" target="_blank" rel="noopener">AWS ParallelCluster&lt;/a> makes it possible and relatively easy to replicate such a setting in the cloud.
ParallelCluster is essentially a wrapper around more basic AWS services such as EC2 (running virtual machines), EFS and EBS (storage), and others.&lt;/p>
&lt;h3 id="pros-and-cons">Pros and Cons&lt;/h3>
&lt;p>Pros&lt;/p>
&lt;ul>
&lt;li>Feasible to set up for a single user or small company / research group without huge resources.&lt;/li>
&lt;li>It scales dynamically: nodes are allocated when needed and decommissioned when unused.&lt;/li>
&lt;li>Practically infinite resources: you can allocate as many nodes and use as much storage as you want.&lt;/li>
&lt;li>It makes it easy to transfer a workflow from a traditional on-premise HPC setting into the cloud.&lt;/li>
&lt;/ul>
&lt;p>Cons&lt;/p>
&lt;ul>
&lt;li>In a university setting you can often get time on HPC systems for free, only requiring a short project description.
Cloud-based HPC will cost you actual money.&lt;/li>
&lt;li>Traditional HPC clusters have qualified systems and network administrators that keep things running smoothly.
With ParallelCluster &lt;em>you&lt;/em> are the systems and network administrator, for better or worse.
While many things are automated, Amazon AWS is nothing if not complex and it still requires knowledge to make things work.&lt;/li>
&lt;/ul>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;p>The first steps of using ParallelCluster are &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3.html" target="_blank" rel="noopener">pretty well documented&lt;/a>.
All tasks are performed with the &lt;code>aws-parallelcluster&lt;/code> Python package.
Note that there are currently two versions of the ParallelCluster software, v2 and v3.
v3 is newer and has more features, but many guides and help you currently find online are based on v2.
The most visual difference is the configuration format: v3 uses &lt;a href="https://en.wikipedia.org/wiki/YAML" target="_blank" rel="noopener">YAML&lt;/a> while v2 uses &lt;a href="https://en.wikipedia.org/wiki/INI_file" target="_blank" rel="noopener">INI&lt;/a> files.&lt;/p>
&lt;ul>
&lt;li>Configure an AWS user with the correct permissions.
A basic policy is &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-example-user-policies" target="_blank" rel="noopener">provided in the AWS documentation&lt;/a>, but
if you need access to services such as EFS more permissions will be needed.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html" target="_blank" rel="noopener">Set up the AWS user&amp;rsquo;s credentials&lt;/a> using &lt;code>aws configure&lt;/code>.&lt;/li>
&lt;li>Either import an SSH public key in the EC2 settings, or generate a new key pair.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-parallelcluster.html" target="_blank" rel="noopener">Install the &lt;code>aws-parallelcluster&lt;/code>&lt;/a> Python package using &lt;code>pip&lt;/code> as documented.
The package is also available through &lt;code>conda&lt;/code> via the conda-forge channel, but as of this writing it is v2 only.&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html" target="_blank" rel="noopener">Set up a cluster configuration&lt;/a> using &lt;code>pcluster configure --config my-cluster-config.yaml&lt;/code>.
This starts an interactive process that various settings of the cluster, i.e. which instance type to use for the head node, which scheduler to use (Slurm or AWS Batch), how many queues, number of nodes per queue, etc.
In particular sets up the necessary virtual private cloud (VPC), network infrastructure, and security groups on AWS automatically, if you tell it to do so.&lt;/li>
&lt;li>Start the cluster using &lt;code>pcluster create-cluster --cluster-name test-cluster --cluster-configuration my-cluster-config.yaml&lt;/code>.
This will take a few minutes; you can check progress using &lt;code>pcluster describe-cluster --cluster-name test-cluster&lt;/code>.&lt;/li>
&lt;li>Log in to the head node using &lt;code>pcluster ssh --cluster-name test-cluster&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="snakemake-on-parallelcluster-with-slurm">Snakemake on ParallelCluster with Slurm&lt;/h2>
&lt;h3 id="peculiarities-of-parallelcluster-slurm-instances">Peculiarities of ParallelCluster Slurm Instances&lt;/h3>
&lt;p>Generally using Snakemake on a ParallelCluster Slurm cluster works the same as with any other Slurm system.
That said, there are some differences and drawbacks, and they are not well documented and as such, are learned through the usual cycle of trial, error, and searching the web.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;code>sbatch&lt;/code> &lt;code>--mem&lt;/code> argument for specifying memory requirements for a job is not supported, and if used will send nodes into the &lt;code>DRAINED&lt;/code> state.
The company Ronin has a good &lt;a href="https://blog.ronin.cloud/slurm-parallelcluster-troubleshooting/" target="_blank" rel="noopener">troubleshooting page&lt;/a> on this topic.
If you are the sole user of your cluster the easiest workaround is probably to scale jobs using the number of CPUs (&lt;code>--cpus-per-task&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>By default, a ParallelCluster comes without working job accounting, which means &lt;code>sacct&lt;/code> does not work.
This is particularly noteworthy since many Snakemake Slurm profiles use &lt;code>sacct&lt;/code> for job monitoring.
A guide for setting up Slurm accounting on ParallelCluster is &lt;a href="https://aws.amazon.com/blogs/compute/enabling-job-accounting-for-hpc-with-aws-parallelcluster-and-amazon-rds/" target="_blank" rel="noopener">available from Amazon&lt;/a>.
If you are going to use ParallelCluster regularly it probably makes sense to set it up, since the underlying database will persist across cluster instances.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="snakemake-slurm-execution-profiles">Snakemake Slurm Execution Profiles&lt;/h3>
&lt;p>The profile I use is freely available on Github at &lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm&lt;/a> and accomodates the ParallelCluster peculiarities.
It is heavily based on &lt;a href="https://github.com/jdblischak/smk-simple-slurm" target="_blank" rel="noopener">John Blischak&amp;rsquo;s smk-simple-slurm repository&lt;/a> and carries the same license.&lt;/p>
&lt;p>Other profiles, such as the &lt;a href="https://github.com/Snakemake-Profiles/slurm" target="_blank" rel="noopener">standard Snakemake Slurm profile&lt;/a> do not currently work in this setting out of the box.&lt;/p>
&lt;h2 id="other-tidbits-and-lessons-learned">Other Tidbits and Lessons Learned&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>There is a &lt;a href="https://github.com/aws-samples/pcluster-manager" target="_blank" rel="noopener">graphical frontend to ParallelCluster&lt;/a>!
It is relatively new and not particularly well advertised in the documentation, but it looks useful and can help avoid potential configuration issues mentioned in this section.
It also simplifies setting up Slurm accounting.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Shared storage&lt;/p>
&lt;ul>
&lt;li>&lt;code>/home&lt;/code> is shared between the head node and the compute nodes.&lt;/li>
&lt;li>&lt;code>/scratch&lt;/code> is the local storage of each node.&lt;/li>
&lt;li>Other shared storage has to be explicity added to the configuration in ParallelCluster v3 (as opposed to v2 where &lt;code>/shared&lt;/code> is a shared EBS volume).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Head node instance type&lt;/p>
&lt;p>Give some thought to which instance type to select for the head node to best balance cost and performance.
Since it is the one cluster node that will run for as long as the cluster is up, the instance type should not be too expensive.
On the other hand it needs to be performant enough for Snakemake, job scheduling and NFS server duties, so do not select an instance type that is too underpowered.
For my applications &lt;code>t3.xlarge&lt;/code> was a good compromise.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check whether the instance types you select exist in your configured AWS region.
Not all instance types are available in all regions, and &lt;code>pcluster&lt;/code> will not warn you about this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check your limits!&lt;/p>
&lt;p>AWS services that ParallelCluster uses may have limits in place, including limits on concurrent EC2 instances and virtual CPUs.
Again, &lt;code>pcluster&lt;/code> will not warn you about this.
In practice this means you can configure and start a cluster that could scale up to hundreds of compute nodes, but when you start a job only a few
nodes spin up while the rest of your jobs are stuck waiting for resources and you are left wondering why.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If things do not work, check the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting-v3.html" target="_blank" rel="noopener">troubleshooting documentation&lt;/a> and the logs.&lt;/p>
&lt;p>On the head node:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">/var/log/cfn-init.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/chef-client.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/slurm_resume.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/slurm_suspend.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/clustermgtd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/slurmctld.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>On compute nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">/var/log/cloud-init-output.log
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/parallelcluster/computemgtd
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">/var/log/slurmd.log
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Ephemeral storage &lt;strong>is ephemeral&lt;/strong>!&lt;/p>
&lt;p>All storage that is created during cluster creation will also be destroyed when the cluster is deleted.
This includes additional EFS and EBS storage defined in the cluster config, unless you explicitly create them yourself first and then include them e.g. via their &lt;code>FileSystemId&lt;/code> for EFS file systems!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Multi-queue clusters&lt;/p>
&lt;p>The &lt;code>pcluster config&lt;/code> wizard enables creating ParallelCluster Slurm clusters with multiple queues.
Keep it mind that it is only possible to specify one instance type per queue.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="resources">Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cbrueffer/snakemake-aws-parallelcluster-slurm" target="_blank" rel="noopener">Snakemake profile for AWS ParallelCluster with Slurm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.aws.amazon.com/parallelcluster" target="_blank" rel="noopener">ParallelCluster documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://aws-parallelcluster.readthedocs.io" target="_blank" rel="noopener">pcluster package documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/aws-samples/pcluster-manager" target="_blank" rel="noopener">ParallelCluster graphical frontend&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hpc.news/techshorts" target="_blank" rel="noopener">Amazon HPC Tech Shorts Youtube channel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=PChP3FQWeJQ" target="_blank" rel="noopener">Amazon HPC Tech Shorts on pcluster manager GUI&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=a-99esKLcls" target="_blank" rel="noopener">Amazon HPC Tech Shorts on pcluster v3&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="edit-history">Edit History&lt;/h2>
&lt;ul>
&lt;li>2022-07-19 Initial version&lt;/li>
&lt;/ul></description></item><item><title>The mutational landscape of the SCAN-B real-world primary breast cancer transcriptome</title><link>https://www.brueffer.io/publication/2020_scanb_mutational_landscape/</link><pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/publication/2020_scanb_mutational_landscape/</guid><description>&lt;html>
&lt;head>
&lt;style>
section {
background: white;
color: black;
border-radius: 1em;
padding: 1em;
left: 50% }
#inner {
display: inline-block;
display: flex;
align-items: center;
justify-content: center }
&lt;/style>
&lt;/head>
&lt;section>
&lt;div id="inner">
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'>&lt;/script>
&lt;span style="float:left";
class="__dimensions_badge_embed__"
data-doi="10.15252/emmm.202012118"
data-hide-zero-citations="true"
data-legend="always">
&lt;/span>
&lt;script async src="https://badge.dimensions.ai/badge.js" charset="utf-8">&lt;/script>
&lt;div style="float:right";
data-link-target="_blank"
data-badge-details="right"
data-badge-type="medium-donut"
data-doi="10.15252/emmm.202012118"
data-condensed="true"
data-hide-no-mentions="true"
class="altmetric-embed">
&lt;/div>
&lt;/div>
&lt;/section></description></item><item><title>A crowdsourced set of curated structural variants for the human genome</title><link>https://www.brueffer.io/publication/2020_giab_sv_curation/</link><pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/publication/2020_giab_sv_curation/</guid><description>&lt;html>
&lt;style>
section {
background: white;
color: black;
border-radius: 1em;
padding: 1em;
left: 50% }
#inner {
display: inline-block;
display: flex;
align-items: center;
justify-content: center }
&lt;/style>
&lt;section>
&lt;div id="inner">
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'>&lt;/script>
&lt;span style="float:left";
class="__dimensions_badge_embed__"
data-doi="10.1371/journal.pcbi.1007933"
data-hide-zero-citations="true"
data-legend="always">
&lt;/span>
&lt;script async src="https://badge.dimensions.ai/badge.js" charset="utf-8">&lt;/script>
&lt;div style="float:right";
data-link-target="_blank"
data-badge-details="right"
data-badge-type="medium-donut"
data-doi="10.1371/journal.pcbi.1007933"
data-condensed="true"
data-hide-no-mentions="true"
class="altmetric-embed">
&lt;/div>
&lt;/div>
&lt;/section></description></item><item><title>SCAN-B MutationExplorer</title><link>https://www.brueffer.io/project/mutationexplorer/</link><pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/project/mutationexplorer/</guid><description>&lt;ul>
&lt;li>URL: &lt;a href="https://oncogenomics.bmc.lu.se/MutationExplorer" target="_blank" rel="noopener">https://oncogenomics.bmc.lu.se/MutationExplorer&lt;/a>&lt;/li>
&lt;li>Source code: &lt;a href="https://github.com/cbrueffer/MutationExplorer" target="_blank" rel="noopener">https://github.com/cbrueffer/MutationExplorer&lt;/a> (2-clause BSD license)&lt;/li>
&lt;/ul>
&lt;p>To facilitate research cancer datasets these days get ever larger. Within the &lt;a href="https://www.scan-b.lu.se/" target="_blank" rel="noopener">Sweden Cancerome Analysis Network&amp;ndash;Breast&lt;/a> thousands of
breast tumors have already undergone &lt;a href="https://en.wikipedia.org/wiki/RNA-Seq" target="_blank" rel="noopener">RNA sequencing&lt;/a>, and the number grows daily. In a &lt;a href="../../publication/scanb_mutational_landscape_primary_bc/">recent research project&lt;/a>
we demonstrated that this data can be used to call somatic mutations, which can e.g. help guide patient treatment or be used as targets for detecting &lt;a href="https://en.wikipedia.org/wiki/Circulating_tumor_DNA" target="_blank" rel="noopener">circulating tumor DNA&lt;/a> in
patient blood samples.&lt;/p>
&lt;p>As part of this project we provide the web portal &lt;a href="https://oncogenomics.bmc.lu.se/MutationExplorer" target="_blank" rel="noopener">SCAN-B Mutationexplorer&lt;/a> to enable the exploration of mutations from 3,217 primary breast tumor transcriptomes.
The underlying software, MutationExplorer, is freely available on GitHub and can be adapted to other datasets.&lt;/p></description></item><item><title>FreeBSD</title><link>https://www.brueffer.io/project/freebsd/</link><pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/project/freebsd/</guid><description>&lt;p>&lt;a href="https://www.freebsd.org" target="_blank" rel="noopener">FreeBSD&lt;/a> is an open source Unix-like operating system that can be
found (as a whole or in parts) in Apple macOS and iOS, Sony Playstation 4, Juniper routers
and NetApp storage filers, to name a few. It also powers services such as Netflix and WhatsApp.
I have been a developer with the project since 2003, and also contribute to related security-focused
projects such as &lt;a href="http://www.trustedbsd.org" target="_blank" rel="noopener">TrustedBSD&lt;/a> and &lt;a href="https://github.com/openbsm" target="_blank" rel="noopener">OpenBSM&lt;/a>.&lt;/p>
&lt;h3 id="commits">Commits&lt;/h3>
&lt;p>&lt;a href="https://github.com/freebsd/freebsd/commits?author=cbrueffer" target="_blank" rel="noopener">to the source code repository&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/freebsd/freebsd-doc/commits?author=cbrueffer" target="_blank" rel="noopener">to the documentation repository&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/freebsd/freebsd-ports/commits?author=cbrueffer" target="_blank" rel="noopener">to the ports repository&lt;/a>&lt;/p></description></item><item><title>Bioconda: sustainable and comprehensive software distribution for the life sciences</title><link>https://www.brueffer.io/publication/2018_bioconda/</link><pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/publication/2018_bioconda/</guid><description>&lt;html>
&lt;style>
section {
background: white;
color: black;
border-radius: 1em;
padding: 1em;
left: 50% }
#inner {
display: inline-block;
display: flex;
align-items: center;
justify-content: center }
&lt;/style>
&lt;section>
&lt;div id="inner">
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'>&lt;/script>
&lt;span style="float:left";
class="__dimensions_badge_embed__"
data-doi="10.1038/s41592-018-0046-7"
data-hide-zero-citations="true"
data-legend="always">
&lt;/span>
&lt;script async src="https://badge.dimensions.ai/badge.js" charset="utf-8">&lt;/script>
&lt;div style="float:right";
data-link-target="_blank"
data-badge-details="right"
data-badge-type="medium-donut"
data-doi="10.1038/s41592-018-0046-7"
data-condensed="true"
data-hide-no-mentions="true"
class="altmetric-embed">
&lt;/div>
&lt;/div>
&lt;/section></description></item><item><title>Biopython Project Update 2017</title><link>https://www.brueffer.io/talk/biopython-project-update-2017/</link><pubDate>Sat, 22 Jul 2017 15:46:00 +0000</pubDate><guid>https://www.brueffer.io/talk/biopython-project-update-2017/</guid><description/></item><item><title>Biopython Project Update 2016</title><link>https://www.brueffer.io/talk/biopython-project-update-2016/</link><pubDate>Sat, 09 Jul 2016 14:00:00 +0000</pubDate><guid>https://www.brueffer.io/talk/biopython-project-update-2016/</guid><description/></item><item><title>TopHat-Recondition: A post-processor for TopHat unmapped reads</title><link>https://www.brueffer.io/publication/2016_tophat-recondition/</link><pubDate>Wed, 04 May 2016 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/publication/2016_tophat-recondition/</guid><description>&lt;html>
&lt;style>
section {
background: white;
color: black;
border-radius: 1em;
padding: 1em;
left: 50% }
#inner {
display: inline-block;
display: flex;
align-items: center;
justify-content: center }
&lt;/style>
&lt;section>
&lt;div id="inner">
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'>&lt;/script>
&lt;span style="float:left";
class="__dimensions_badge_embed__"
data-doi="10.1186/s12859-016-1058-x"
data-hide-zero-citations="true"
data-legend="always">
&lt;/span>
&lt;script async src="https://badge.dimensions.ai/badge.js" charset="utf-8">&lt;/script>
&lt;div style="float:right";
data-link-target="_blank"
data-badge-details="right"
data-badge-type="medium-donut"
data-doi="10.1186/s12859-016-1058-x"
data-condensed="true"
data-hide-no-mentions="true"
class="altmetric-embed">
&lt;/div>
&lt;/div>
&lt;/section></description></item><item><title>Practical Security Event Auditing with FreeBSD</title><link>https://www.brueffer.io/talk/practical-security-event-auditing-with-freebsd/</link><pubDate>Sun, 14 Nov 2010 10:00:00 +0000</pubDate><guid>https://www.brueffer.io/talk/practical-security-event-auditing-with-freebsd/</guid><description/></item><item><title>FreeBSD Security Event Auditing</title><link>https://www.brueffer.io/publication/2009_bsd_magazine_auditing/</link><pubDate>Wed, 01 Apr 2009 00:00:00 +0000</pubDate><guid>https://www.brueffer.io/publication/2009_bsd_magazine_auditing/</guid><description/></item><item><title>Protecting your Privacy with FreeBSD and Tor</title><link>https://www.brueffer.io/talk/protecting-your-privacy-with-freebsd-and-tor/</link><pubDate>Sat, 17 Nov 2007 15:00:00 +0000</pubDate><guid>https://www.brueffer.io/talk/protecting-your-privacy-with-freebsd-and-tor/</guid><description/></item><item><title>FreeBSD im Überblick</title><link>https://www.brueffer.io/talk/freebsd-im-uberblick/</link><pubDate>Sat, 24 Jun 2006 11:00:00 +0000</pubDate><guid>https://www.brueffer.io/talk/freebsd-im-uberblick/</guid><description/></item><item><title>What is FreeBSD?</title><link>https://www.brueffer.io/talk/what-is-freebsd/</link><pubDate>Fri, 25 Jun 2004 13:00:00 +0000</pubDate><guid>https://www.brueffer.io/talk/what-is-freebsd/</guid><description/></item></channel></rss>